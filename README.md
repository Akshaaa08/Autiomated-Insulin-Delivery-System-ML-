Automated Insulin Delivery System â€” Glucose Prediction using Temporal Fusion Transformer (TFT)

This project implements a machine learningâ€“based glucose prediction system designed to assist in Automated Insulin Delivery (AID) for Type 1 Diabetes patients.
Using real patient data from the OhioT1DM dataset, the model predicts future blood glucose levels based on historical glucose, insulin, and meal information.

The system integrates multiple deep learning architectures â€” GRU, Transformer, and the final Temporal Fusion Transformer (TFT) â€” and compares their performance using both statistical and clinical metrics.

ğŸ“š Table of Contents

Overview

Key Features

Dataset

System Workflow

Model Architectures

Evaluation Metrics

Results Summary

Error Analysis

Setup Instructions

Repository Structure

Future Improvements

Contributors

ğŸ§  Overview

The goal of this project is to accurately predict a patientâ€™s blood glucose level 30â€“60 minutes ahead and integrate this predictive capability into an automated insulin delivery loop.
This helps in preventing hypoglycemia and hyperglycemia by adjusting insulin dosage in real time.

The project explores multiple models:

GRU (Baseline) â€” Sequential model capturing short-term dependencies.

Transformer (Intermediate) â€” Attention-based model for long-range temporal patterns.

TFT (Final Model) â€” Combines LSTM + attention + variable selection + quantile forecasting for interpretability and robustness.

âš™ï¸ Key Features

ğŸ“ˆ Real-time blood glucose forecasting using deep learning

ğŸ” Temporal pattern learning from meal, insulin, and glucose histories

ğŸ“Š Clinical-grade evaluation via Clarke Error Grid Analysis

ğŸ’¬ Explainable forecasts via attention and variable importance

ğŸ§® Ensemble training and model checkpointing with PyTorch Lightning

â˜ï¸ Scalable design for cloud deployment and integration with IoT glucose sensors

ğŸ“‚ Dataset

Source: OhioT1DM dataset (open-source clinical dataset)

Files: 12 XML files (559-ws-training.xml, 563-ws-testing.xml, etc.)

Patients: 6 Type 1 diabetic individuals

Sampling: 5-minute glucose, insulin, and carbohydrate intake logs

Sensors: Continuous Glucose Monitor (CGM), Insulin pump, Meal entries

Data Fields Extracted:

Glucose (mg/dL)

Insulin bolus (U)

Carbohydrates (grams)

Timestamp

Derived features â€” glucose velocity, rolling mean/std, time since meal/bolus

ğŸ”„ System Workflow
1ï¸âƒ£ XML Parsing â†’ Extract glucose, insulin, and meal events
2ï¸âƒ£ Data Alignment â†’ Resample at 5-min intervals
3ï¸âƒ£ Feature Engineering â†’ Rolling stats, velocity, event lags
4ï¸âƒ£ Scaling â†’ Normalization of real-valued features
5ï¸âƒ£ Dataset Creation â†’ TimeSeriesDataSet for TFT
6ï¸âƒ£ Model Training â†’ GRU / Transformer / TFT
7ï¸âƒ£ Evaluation â†’ MAE, RMSE, Clarke Error Grid
8ï¸âƒ£ Visualization â†’ Predictions, Error Zones, Clinical Insights

ğŸ§© Model Architectures
1. GRU (Gated Recurrent Unit)

Simple recurrent model capturing short-term glucose trends

Limitation: Poor at modeling meal/insulin dependencies

MAE: 28.4 mg/dL | RMSE: 35.2 mg/dL | A+B Zones: 52.6%

2. Transformer

Uses self-attention to capture long-term dependencies

Improved temporal awareness compared to GRU

MAE: 23.8 mg/dL | RMSE: 30.4 mg/dL | A+B Zones: 61.8%

3. Temporal Fusion Transformer (TFT)

Combines LSTM encoder-decoder + attention + variable selection

Learns temporal dependencies and feature importance dynamically

MAE: 14.28 mg/dL | RMSE: 20.27 mg/dL | A+B Zones: 70.2%

ğŸ“ Evaluation Metrics
Metric	Description
MAE	Mean Absolute Error â€” measures average deviation
RMSE	Root Mean Squared Error â€” penalizes large deviations
Clarke Error Grid	Evaluates clinical safety of predictions
A+B Zone Accuracy	% of predictions within clinically acceptable range
ğŸ§ª Results Summary
Model	MAE	RMSE	A+B Zones (%)
GRU	28.47	35.19	52.6
Transformer	23.82	30.44	61.8
TFT (Final)	14.28	20.27	70.2

ğŸ©º Clinical Interpretation:

TFT achieved the highest medical safety, with ~70% of predictions in no-risk zones.

The model effectively handled missing event data and maintained stability across patient profiles.

ğŸ” Error Analysis

GRU: Over-smoothed predictions; struggled during rapid glucose fluctuations.

Transformer: Better temporal learning; mild overfitting on limited data.

TFT: Robust, interpretable, and generalized better; reduced bias and variance.

Clarke Error Grid visualization shows:

GRU predictions widely dispersed.

Transformer predictions closer to the diagonal.

TFT predictions tightly clustered around the line of identity.

ğŸ› ï¸ Setup Instructions
Prerequisites

Python 3.10+

Install dependencies:

pip install -r requirements.txt

Running the Pipeline

Clone the repo:

git clone https://github.com/<your-username>/Glucose-Prediction-TFT.git
cd Glucose-Prediction-TFT


Launch the notebook:

jupyter notebook Final_ML_Project.ipynb


Or run end-to-end:

python main_pipeline.py

Output Files

best_model.ckpt â€” trained TFT model

glucose_scaler.joblib â€” target scaler

real_features_scaler.joblib â€” feature scaler

Clarke Error Grid plots and evaluation report

ğŸ“ Repository Structure
ğŸ“¦ Glucose-Prediction-TFT/
 â”£ ğŸ“„ Final_ML_Project.ipynb          # Main notebook
 â”£ ğŸ“„ GRU_Glucose_Prediction.ipynb    # GRU baseline
 â”£ ğŸ“„ Transformer_Model.ipynb         # Transformer model
 â”£ ğŸ“„ install_dependencies.py
 â”£ ğŸ“„ glucose_scaler.joblib
 â”£ ğŸ“„ real_features_scaler.joblib
 â”£ ğŸ“„ best_model.ckpt
 â”£ ğŸ“„ README.md
 â”£ ğŸ“‚ lightning_logs/
 â”£ ğŸ“‚ dataset/                        # XML patient data
 â”— ğŸ“Š results/                        # Graphs & error plots

ğŸš€ Future Improvements

Integrate real-time CGM and insulin pump APIs

Implement TFT with attention visualization dashboard

Extend to hybrid cloud + embedded inference

Incorporate federated learning for patient privacy (HIPAA-compliant)

ğŸ‘©â€ğŸ’» Contributors

Project Lead: Akssss
Guided by: [Your Faculty/Guide Name]
Technologies: PyTorch Lightning, Pandas, Matplotlib, Scikit-learn
